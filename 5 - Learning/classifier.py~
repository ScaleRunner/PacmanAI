def flatten(l):
    """
    Utility function: flatten a multiply nested list of tuple into a single layer of nesting.
    """
    if isinstance(l, (list, tuple)):
        out = []
        for item in l:
            out.extend(flatten(item))
        return out
    else:
        return [l]

def classification_error(labels,predictions):
    """ Compute the classification error, which is the fraction of points for which the prediction does not equal the true label
    """
    M = min(len(labels),len(predictions))
    n_mispts = 0
    for i in range(0,M):
        if int(predictions[i]) != int(labels[i]):
            n_mispts += 1
    error = n_mispts / float(M)
    return error

class BinaryClassifier:
    def __init__(self):
        """
        Classifier initialization.
        Put any code here that you need to initialize parameters
        before being trained, e.g. self.W=0, self.threshold=0...
        """
        self.weight = 1
        self.threshold = -1
        self.alpha = 0.6
        "*** YOUR CODE HERE (assignment 1) ***"

    def train(self,data,labels):
        """ 
        train a classifier to predict the labels from the data
        data is a list of lists of features, i.e. 1 element per example
        labels is a list of binary labels (0 or 1)

        Note: data[i] is a 2-d list of lists where with each list containing each
            feature for a given time point.
            Thus data[i][t][f] is the f'th feature of
                the t'th time-point of
                the i'th example.
            To simplify your training method you probably want to *flatten* each example to make it a simple list of features before usage, e.g. data_i = flatten(data[i]) 
            For each time-point there are 4 features which are the x,y locations of the
                pacman and the ghost in the order : [pac_x pac_y ghost_x ghost_y]
        """
        # loop over all examples 10 times,
        #  Note: you probably want a better test for when to stop learning)
        self.weightvector = [self.weight for a in range(len(data[0]))]
        output = [False for a in range(len(data))]
        for cycle in range(0,10):
            # loop over all examples
            #  Note: you may want to process the points in a different order..
            for i in range(0,len(data)):
                data_i = data[i]
                label_i= labels[i]
                oldvector = self.weightvector
                output[i] = self.output(data_i)
                #print(label_i == self.output(data_i))
                for x in range(len(data_i)):
                    self.weightvector[x] = oldvector[x] + (self.alpha * (label_i - self.output(data_i)) * data_i[x])

                #print(data_i, label_i)
                # print("{}.{}) X={}feat Y={}".format(cycle,i,len(data_i),label_i))
                # do some learning on this example, if it is needed....
                
                "*** YOUR CODE HERE ***"
            print(output)
            # print some debug information about classifier performance
            err=classification_error(labels,output)
            print("%d) error rate = %f" % (cycle,err))

    def output(self,example):
        """ generate a prediction for a single example """
        label = 0
        index = 0
        for feature in example:
            label += feature*self.weightvector[index]
            index += 1
        label += self.threshold
        if label < 0:
            return 0
        else:
            return 1

    def test(self,examples):
        """ generate a prediction for each of the input examples """
        print(self.weightvector)
        return [self.output(example) for example in examples]


def identityfeaturemapper(sample):
    """
    The identity feature mapper is the default
    feature mapper: it does not change the sample.
    It also flattens the sample so that it is
    one-dimensional and can be processed by the classifier.
    """
    return flatten(sample)

def pacmanfeaturemapper(sample):
    """
    Given an input sample consisting of data,
    apply a feature transformation to return a new set
    which makes the classification problem easier.
    The returned value must be one-dimension (i.e. a list of values)

    A sample is a list of past time-points.
    Each time-point is a list of features: [pac_x, pac_y, ghost_x, ghost_y]
    """

    "*** YOUR CODE HERE (assignment 2) ***"

    raise NotImplementedError("You need to write this as part of assignment 2")


"*** YOUR CODE HERE (Bonus Assignment) ***"
class MultiClassClassifier:
    def __init__(self):
        """
        Classifier initialization.
        Put any code here that you need to initialize parameters
        before being trained, e.g. self.W=0, self.threshold=0...
        """

        "*** YOUR CODE HERE (assignment 1) ***"

    def train(self,data,labels):
        """ 
        train a classifier to predict the labels from the data
        data is a list of lists of features, i.e. 1 element per example
        labels is a list of integer class lables for each example [0,1,2,...]
        """
        "*** YOUR CODE HERE (bonus assignment) ***"

    def output(self,example):
        """ generate a prediction for a single example """

        "*** YOUR CODE HERE (bonus assignment) ***"
        return 0

    def test(self,examples):
        """ generate a prediction for each of the input examples """
        return [self.output(example) for example in examples]
